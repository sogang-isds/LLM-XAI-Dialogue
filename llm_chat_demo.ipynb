{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956ff3a4",
   "metadata": {
    "id": "956ff3a4"
   },
   "source": [
    "# Chat\n",
    "\n",
    "Recall the overall workflow for retrieval augmented generation (RAG):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98fb4f3",
   "metadata": {
    "id": "d98fb4f3"
   },
   "source": [
    "We discussed `Document Loading` and `Splitting` as well as `Storage` and `Retrieval`.\n",
    "\n",
    "We then showed how `Retrieval` can be used for output generation in Q+A using `RetrievalQA` chain."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install openai langchain \"langchain[docarray]\" pypdf chromadb tiktoken"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_lpt_xfh06T",
    "outputId": "13d0b8d6-f3e9-46a0-fe08-28ba689b0768"
   },
   "id": "v_lpt_xfh06T",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fc482-dd03-40e1-8083-171aad3e2b26",
   "metadata": {
    "tags": [],
    "id": "926fc482-dd03-40e1-8083-171aad3e2b26"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# import panel as pn  # GUI\n",
    "# pn.extension()\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "key = 'OPENAI_API_KEY'\n",
    "api_key = 'sk-' + key + 'bkFJm28ZY54dRNHk3u5edkod'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ac838-35f7-48f6-bcb1-f259057efb0b",
   "metadata": {
    "id": "a75ac838-35f7-48f6-bcb1-f259057efb0b"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f236620-f56f-4c6e-88d0-7741c0327522",
   "metadata": {
    "id": "3f236620-f56f-4c6e-88d0-7741c0327522"
   },
   "source": [
    "If you wish to experiment on `LangChain plus platform`:\n",
    "\n",
    " * Go to [langchain plus platform](https://www.langchain.plus/) and sign up\n",
    " * Create an api key from your account's settings\n",
    " * Use this api key in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83558969-e5aa-4fbc-933c-9c1534decf0a",
   "metadata": {
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83558969-e5aa-4fbc-933c-9c1534decf0a",
    "outputId": "de0b142e-ba3c-421d-9d28-aafdca745bce"
   },
   "outputs": [],
   "source": [
    "llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c994176-0b0e-4797-a83a-e955fd0fecd9",
   "metadata": {
    "id": "9c994176-0b0e-4797-a83a-e955fd0fecd9"
   },
   "source": [
    "## LLM Chat: Memoryless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81762a4e-f8cb-4f1b-9c1b-bf2cde94fc34",
   "metadata": {
    "id": "81762a4e-f8cb-4f1b-9c1b-bf2cde94fc34"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98698fa-5f16-4f14-87f6-b63233e7161a",
   "metadata": {
    "id": "c98698fa-5f16-4f14-87f6-b63233e7161a"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "full_prompt = PromptTemplate.from_template(\n",
    "    template=\"<s>[INST]<<SYS>>{sys_msg}<</SYS>>\\n\\nContext:\\n{history}\\n\\nHuman: {input}\\n[/INST] {primer}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15575805-badd-4176-9c5f-f4ca8741fa58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15575805-badd-4176-9c5f-f4ca8741fa58",
    "outputId": "9d1404c4-434a-4a44-b552-0c3443f4f74c"
   },
   "outputs": [],
   "source": [
    "prompt = full_prompt.partial(\n",
    "    sys_msg = (\n",
    "        \"You are a helpful, respectful and honest AI assistant.\"\n",
    "        \"\\nAlways answer as helpfully as possible, while being safe.\"\n",
    "        \"\\nPlease be brief and efficient unless asked to elaborate, and follow the conversation flow.\"\n",
    "        \"\\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\"\n",
    "        \"\\nEnsure that your responses are socially unbiased and positive in nature.\"\n",
    "        \"\\nIf a question does not make sense or is not factually coherent, explain why instead of answering something incorrect.\"\n",
    "        \"\\nIf you don't know the answer to a question, please don't share false information.\"\n",
    "        \"\\nIf the user asks for a format to output, please follow it as closely as possible.\"\n",
    "    ),\n",
    "    primer = \"\",\n",
    "    history = \"\",\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Help me with my homework\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e6652-9b25-47e6-98b4-e7320692123b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "432e6652-9b25-47e6-98b4-e7320692123b",
    "outputId": "8cbbfa7b-2506-44a0-ca4d-f75ce4e970db"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt= prompt, verbose=True)\n",
    "print(chain.run(input=\"Hello World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19afcd7-4ebe-48c5-80f4-fb8d8e523f69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "b19afcd7-4ebe-48c5-80f4-fb8d8e523f69",
    "outputId": "f7026aa4-e274-4aca-8c97-908eef544036"
   },
   "outputs": [],
   "source": [
    "chain.run(input=\"What was the first thing I asked you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf48cff-f63f-40e6-a05c-280891703727",
   "metadata": {
    "id": "acf48cff-f63f-40e6-a05c-280891703727"
   },
   "source": [
    "## LLM Chat: Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee6f2d-1789-4980-a334-06e562c041da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "5bee6f2d-1789-4980-a334-06e562c041da",
    "outputId": "cb2d8e54-2386-48f2-c490-5095583ff59c"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "hist_prompt = prompt.copy()\n",
    "#print(\"hist=\",hist_prompt)\n",
    "hist_prompt.input_variables = ['input', 'history']\n",
    "#print(\"hist2=\",hist_prompt)\n",
    "\n",
    "conv_chain = ConversationChain(llm=llm, prompt=hist_prompt, verbose=True)\n",
    "conv_chain.run(input=\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3721ddb-b5c3-4135-912c-7e70449b1130",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "e3721ddb-b5c3-4135-912c-7e70449b1130",
    "outputId": "7c18929d-daf2-46c8-ab41-3d9ebd6ee237"
   },
   "outputs": [],
   "source": [
    "conv_chain.run(input=\"What was the first thing I asked you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ee0ea-8e91-4467-867b-b6914106d2a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b30ee0ea-8e91-4467-867b-b6914106d2a7",
    "outputId": "867f2520-dcce-45cf-fa8c-1ade83af0645"
   },
   "outputs": [],
   "source": [
    "conv_chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbacc260-d168-4a07-ac6a-272a2834a83c",
   "metadata": {
    "id": "fbacc260-d168-4a07-ac6a-272a2834a83c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc94c49a-dd80-47f5-aa8f-7310b93f9145",
   "metadata": {
    "id": "cc94c49a-dd80-47f5-aa8f-7310b93f9145"
   },
   "source": [
    "### RAG + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b2f0c-80ef-4e81-8142-d114ecd809c2",
   "metadata": {
    "id": "4b7b2f0c-80ef-4e81-8142-d114ecd809c2"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"sample_docs/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"sample_docs/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"sample_docs/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"sample_docs/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb6d95-2629-4cd2-a8fb-87f29eb4023a",
   "metadata": {
    "id": "fceb6d95-2629-4cd2-a8fb-87f29eb4023a"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636d00a-f86d-479e-835f-b527493ddefe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0636d00a-f86d-479e-835f-b527493ddefe",
    "outputId": "9256148d-2a3e-4a79-91ca-015570ee970d"
   },
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7545ac9-4dcc-4b3a-a13f-371b0f31d784",
   "metadata": {
    "id": "c7545ac9-4dcc-4b3a-a13f-371b0f31d784"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa8729-a5a9-4163-8a2f-3d2915bf3cc6",
   "metadata": {
    "id": "baaa8729-a5a9-4163-8a2f-3d2915bf3cc6"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24006c9-3ce8-4de1-8a80-0b510230b2d9",
   "metadata": {
    "id": "b24006c9-3ce8-4de1-8a80-0b510230b2d9"
   },
   "source": [
    "## Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aec055-775a-44fa-8e05-5bb37ec9ac4d",
   "metadata": {
    "id": "87aec055-775a-44fa-8e05-5bb37ec9ac4d"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef140f-6c8f-4ae8-a6b2-9a856f8d4579",
   "metadata": {
    "id": "dfef140f-6c8f-4ae8-a6b2-9a856f8d4579"
   },
   "source": [
    "### similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a409bf",
   "metadata": {
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7a409bf",
    "outputId": "42123716-a6e6-4f10-985f-d34bd73f2157"
   },
   "outputs": [],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "print(docs[0], \"\\n\\n\", docs[1], \"\\n\\n\", docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6bf6a-ea47-46a7-a036-584dcd756af4",
   "metadata": {
    "id": "76d6bf6a-ea47-46a7-a036-584dcd756af4"
   },
   "source": [
    "### mmr   : max_marginal_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fa50d-4660-4d6e-8ab2-7343fc65d36c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e07fa50d-4660-4d6e-8ab2-7343fc65d36c",
    "outputId": "f89cdc5e-9df4-42c9-85b5-f5bb146da744"
   },
   "outputs": [],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.max_marginal_relevance_search(question, k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccacb0-c599-4a4b-b157-aa5ec6968768",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eccacb0-c599-4a4b-b157-aa5ec6968768",
    "outputId": "5730df87-41b2-4bc0-ec5e-14862ef63bb2"
   },
   "outputs": [],
   "source": [
    "print(docs[0], \"\\n\\n\", docs[1], \"\\n\\n\", docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c58c2b",
   "metadata": {
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "01c58c2b",
    "outputId": "37cb0d3c-5d31-4855-acae-100757180546"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0, api_key=api_key)\n",
    "llm.predict(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbcc89-61cf-4a79-ac83-336dd2aef26f",
   "metadata": {
    "id": "34bbcc89-61cf-4a79-ac83-336dd2aef26f"
   },
   "source": [
    "## RetrivalQA1: Similarity"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
    "\n",
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "question = \"Is probability a class topic?\"\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(\\\n",
    "                                           search_type='similarity',\\\n",
    "                                        search_kwargs={'k': 3},\\\n",
    "                                       return_source_documents=True,\\\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}))\n",
    "\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LEf3G72dlw51",
    "outputId": "2b7a8fc7-9669-4734-c65d-e66fe701643d"
   },
   "id": "LEf3G72dlw51",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ef3c492b-9666-4117-a762-706fb5e0589b",
   "metadata": {
    "id": "ef3c492b-9666-4117-a762-706fb5e0589b"
   },
   "source": [
    "## RetrivalQA2: mmr : max_marginal_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa6c11",
   "metadata": {
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "b8aa6c11",
    "outputId": "65ab3d54-ac9d-40f9-9efc-f146ff02e123"
   },
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
    "\n",
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "question = \"Is probability a class topic?\"\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(\\\n",
    "                                           search_type='mmr',\\\n",
    "                                        search_kwargs={'k': 3},\\\n",
    "                                       return_source_documents=True,\\\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}))\n",
    "\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c337805-a1ab-4147-80b3-ce9764533584",
   "metadata": {
    "id": "9c337805-a1ab-4147-80b3-ce9764533584"
   },
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99e754",
   "metadata": {
    "tags": [],
    "id": "7e99e754"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe856abf-65a2-49fb-9256-d9604e9850f2",
   "metadata": {
    "id": "fe856abf-65a2-49fb-9256-d9604e9850f2"
   },
   "source": [
    "### ConversationalRetrievalChain 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50090a72-c820-45b6-ad3e-a0a36297d16f",
   "metadata": {
    "id": "50090a72-c820-45b6-ad3e-a0a36297d16f"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd6466-d8bd-4455-83fd-73f1de0f62d4",
   "metadata": {
    "id": "64dd6466-d8bd-4455-83fd-73f1de0f62d4"
   },
   "source": [
    "### ConversationalRetrievalChain 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23700502",
   "metadata": {
    "tags": [],
    "id": "23700502"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever(search_type='mmr',search_kwargs={'k': 3})\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b9fbb",
   "metadata": {
    "tags": [],
    "id": "e25b9fbb"
   },
   "outputs": [],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622478ba",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "622478ba",
    "outputId": "8958a815-5332-4003-f052-790144f55d36"
   },
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b280672",
   "metadata": {
    "tags": [],
    "id": "5b280672"
   },
   "outputs": [],
   "source": [
    "question = \"why are those prerequesites needed?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd08a60",
   "metadata": {
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "fcd08a60",
    "outputId": "ca9d7c57-2c83-4f16-df91-b117a579f11b"
   },
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599c912-d9cb-4cf2-84fc-70ff9ddf68e2",
   "metadata": {
    "tags": [],
    "id": "e599c912-d9cb-4cf2-84fc-70ff9ddf68e2"
   },
   "source": [
    "# Create a chatbot on same documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622a19c-0711-49aa-8d1d-9ddfb6af7032",
   "metadata": {
    "tags": [],
    "id": "8622a19c-0711-49aa-8d1d-9ddfb6af7032"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e119e-7561-49ef-be37-6d6a904752af",
   "metadata": {
    "id": "9a9e119e-7561-49ef-be37-6d6a904752af"
   },
   "source": [
    "The chatbot code has been updated a bit since filming. The GUI appearance also varies depending on the platform it is running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a41222-8154-4492-95b0-b3fd1083cf5a",
   "metadata": {
    "tags": [],
    "id": "d8a41222-8154-4492-95b0-b3fd1083cf5a"
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "chain_type = \"stuff\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# define embedding\n",
    "embeddings = OpenAIEmbeddings(api_key=api_key)\n",
    "# create vector database from data\n",
    "db = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "# define retriever\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "# create a chatbot chain. Memory is managed externally.\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(model_name=llm_name, temperature=0, api_key=api_key),\n",
    "#    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525019d-3acd-40ff-9844-141116155d68",
   "metadata": {
    "tags": [],
    "id": "b525019d-3acd-40ff-9844-141116155d68"
   },
   "outputs": [],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc1b51-488d-4a68-80a3-e811deebf62f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "4edc1b51-488d-4a68-80a3-e811deebf62f",
    "outputId": "3d62721a-64f0-474c-bd1d-00b580b6382e"
   },
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e72172-5f6b-4059-8cc4-43e69504483c",
   "metadata": {
    "id": "34e72172-5f6b-4059-8cc4-43e69504483c"
   },
   "outputs": [],
   "source": [
    "question = \"why are those prerequesites needed?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6022a3-5095-40ca-8434-072fc5969cfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "4d6022a3-5095-40ca-8434-072fc5969cfe",
    "outputId": "8b960107-a622-4436-adee-b05791f8b9b7"
   },
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
