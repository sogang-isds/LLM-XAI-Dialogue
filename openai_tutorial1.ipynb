{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OpenAI API 및 LangChain 사용하기\n",
    "\n",
    "## OpenAI API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install openai==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # 환경변수에 OPENAI_API_KEY를 설정합니다.\n",
    "\n",
    "client = OpenAI(\n",
    "    # Defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text Completion 예제\n",
    "\n",
    "기본적인 텍스트 생성 예제입니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"안녕? 넌 누구니?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "response = client.completions.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\", # 사용할 AI 모델을 지정합니다. 여기서는 \"text-davinci-003\" 모델을 사용하였습니다.\n",
    "  prompt=\"안녕? 넌 누구니?\", # AI 모델에게 전달할 프롬프트를 지정합니다.\n",
    "  max_tokens=256, # 생성될 텍스트의 최대 토큰 수를 지정합니다. 여기서는 256개의 토큰을 지정하였습니다.\n",
    "  temperature=0 # 출력의 다양성을 조절합니다. 값이 0이면 가장 확률이 높은 텍스트를 생성하고, 값이 1에 가까울수록 더 다양한 텍스트를 생성합니다.\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response.choices[0].text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat Completion 예제"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\", # 사용할 AI 모델을 지정합니다. 여기서는 \"text-davinci-003\" 모델을 사용하였습니다.\n",
    "  messages=[ # AI 모델에게 전달할 프롬프트를 지정합니다.\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"안녕? 넌 누구니?\",\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=256, # 생성될 텍스트의 최대 토큰 수를 지정합니다. 여기서는 256개의 토큰을 지정하였습니다.\n",
    "  temperature=0 # 출력의 다양성을 조절합니다. 값이 0이면 가장 확률이 높은 텍스트를 생성하고, 값이 1에 가까울수록 더 다양한 텍스트를 생성합니다.\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### role을 지정하여 대화하기\n",
    "\n",
    "role 종류\n",
    "- system: 시스템의 목적을 정의할 때 사용\n",
    "- assistant: assistant의 응답 메시지를 저장하는데 사용\n",
    "- user: 사용자의 입력을 넘길 때 사용\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that like google assistant or siri.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"안녕? 넌 누구니?\",\n",
    "    },\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\", # 사용할 AI 모델을 지정합니다. 여기서는 \"text-davinci-003\" 모델을 사용하였습니다.\n",
    "  messages=example_messages, # AI 모델에게 전달할 프롬프트를 지정합니다.\n",
    "  max_tokens=256, # 생성될 텍스트의 최대 토큰 수를 지정합니다. 여기서는 256개의 토큰을 지정하였습니다.\n",
    "  temperature=0 # 출력의 다양성을 조절합니다. 값이 0이면 가장 확률이 높은 텍스트를 생성하고, 값이 1에 가까울수록 더 다양한 텍스트를 생성합니다.\n",
    ")\n",
    "\n",
    "response_message_1 = response.choices[0].message.content\n",
    "print(response_message_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 대화를 이어나가기\n",
    "\n",
    "이전에 받은 대화를 assistant의 입력으로 사용하여 대화를 이어나갈 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response_message_1,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"2020년 올림픽은 개최국은 어디야?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "new_messages = example_messages.copy()\n",
    "new_messages.extend(next_messages)\n",
    "from pprint import pprint\n",
    "pprint(new_messages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\", # 사용할 AI 모델을 지정합니다. 여기서는 \"text-davinci-003\" 모델을 사용하였습니다.\n",
    "  messages=new_messages, # AI 모델에게 전달할 프롬프트를 지정합니다.\n",
    "  max_tokens=256, # 생성될 텍스트의 최대 토큰 수를 지정합니다. 여기서는 256개의 토큰을 지정하였습니다.\n",
    "  temperature=0 # 출력의 다양성을 조절합니다. 값이 0이면 가장 확률이 높은 텍스트를 생성하고, 값이 1에 가까울수록 더 다양한 텍스트를 생성합니다.\n",
    ")\n",
    "response_message_2 = response.choices[0].message.content\n",
    "print(response_message_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 대화의 맥락을 유지하는지 확인하기\n",
    "\n",
    "이전 대화 맥락을 고려하여 답변을 하고 있는지 확인하기 위해 짧은 입력만 주고 대답을 받아봅니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response_message_2,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"언제 열려?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "new_messages.extend(next_messages)\n",
    "pprint(new_messages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\", # 사용할 AI 모델을 지정합니다. 여기서는 \"text-davinci-003\" 모델을 사용하였습니다.\n",
    "  messages=new_messages, # AI 모델에게 전달할 프롬프트를 지정합니다.\n",
    "  max_tokens=256, # 생성될 텍스트의 최대 토큰 수를 지정합니다. 여기서는 256개의 토큰을 지정하였습니다.\n",
    "  temperature=0 # 출력의 다양성을 조절합니다. 값이 0이면 가장 확률이 높은 텍스트를 생성하고, 값이 1에 가까울수록 더 다양한 텍스트를 생성합니다.\n",
    ")\n",
    "response_message_3 = response.choices[0].message.content\n",
    "print(response_message_3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LangChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.350"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat 모델 사용하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 단순 대화하기\n",
    "\n",
    "메시지는 HumanMessage를 이용해 생성합니다. 이 메시지를 chat_llm에 전달하면 AI 모델이 답변을 생성합니다.\n",
    "OpenAI API를 직접 사용하는 것과 마찬가지로, 질의를 보낼 때마다 새로운 세션으로 동작합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = chat_llm([HumanMessage(content=\"안녕? 넌 누구니?\")])\n",
    "print(result.content)\n",
    "result_content_1 = result.content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = chat_llm([HumanMessage(content=\"2020년 서울의 인구를 알려줘\")])\n",
    "print(result.content)\n",
    "result_content_2 = result.content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = chat_llm([HumanMessage(content=\"그럼 부산은?\")])\n",
    "print(result.content)\n",
    "result_content_3 = result.content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 대화의 이력을 통해 대화하기\n",
    "\n",
    "위와 동일한 대화를 보내 맥락을 잘 유지하고 답변을 하는지 확인합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chat_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that like google assistant or siri.\"),\n",
    "    HumanMessage(content=\"안녕? 넌 누구니?\"),\n",
    "    AIMessage(content=result_content_1),\n",
    "    HumanMessage(content=\"2020년 서울의 인구를 알려줘\"),\n",
    "    AIMessage(content=result_content_2),\n",
    "    HumanMessage(content=\"그럼 부산은?\"),\n",
    "    AIMessage(content=result_content_3),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = chat_llm(chat_messages)\n",
    "print(result.content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 토큰 수 확인하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chat_llm.get_num_tokens('안녕? 넌 누구니?')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chat_llm.get_num_tokens_from_messages(chat_messages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batch 단위로 메시지 보내기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that like google assistant or siri.\"),\n",
    "        HumanMessage(content=\"2020년 서울의 인구를 알려줘\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that like google assistant or siri.\"),\n",
    "        HumanMessage(content=\"그럼 부산은?\")\n",
    "    ]\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = chat_llm.generate(batch_messages)\n",
    "\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 생성 결과를 출력"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for idx, generation in enumerate(result.generations):\n",
    "    print(f'batch index: {idx}, response message: {generation[0].text}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 토큰 사용량 출력"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(result.llm_output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 스트리밍 요청"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = chat([HumanMessage(content=\"지구온난화에 대해 짧게 설명해줘\")])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat Prompt Template 사용하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "response = chat_llm(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\").to_messages())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print([response])  # 객체를 출력하기 위해 리스트 형태로 감쌈\n",
    "print('content:', response.content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chain 사용하기\n",
    "\n",
    "#### LLM과 prompt 템플릿를 함께 사용하기\n",
    "\n",
    "위와 동일한 대화를 `LLMChain`를 이용해 생성합니다. `PromptTemplate` 객체를 `to_messages()`로 변환하는 과정이 필요 없이 바로 `LLMChain`에 넘기면 됩니다. 응답 결과는 객체가 아닌 텍스트인 부분이 다릅니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(llm=chat_llm, prompt=chat_prompt)\n",
    "response = llm_chain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 수학 계산용 체인 - LLMMathChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain.from_llm(llm=chat_llm, verbose=True)\n",
    "\n",
    "math_chain.run(\"What is 2 raised to the 10 power?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 대화용 체인 - ConversationChain\n",
    "\n",
    "대화용 체인은 local memory 객체에 대화의 이력을 저장하고 관리한다. 이전에 수동으로 이력을 관리하던 방식과 달리, 이력을 관리하는 부분을 자동으로 처리한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "conversation_chain = ConversationChain(llm=chat_llm, verbose=True)\n",
    "conversation_chain.predict(input=\"안녕? 넌 누구니?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conversation_chain.predict(input=\"2020년 서울의 인구를 알려줘\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conversation_chain.predict(input=\"그럼 부산은?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Few shot 에제\n",
    "\n",
    "HumanMessage와 AIMessage를 이용해 대화를 반복하여 incontext learning을 위한 예시를 넘길 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=chat_llm, prompt=chat_prompt)\n",
    "# get a chat completion from the formatted messages\n",
    "chain.run(text=\"I love programming.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
